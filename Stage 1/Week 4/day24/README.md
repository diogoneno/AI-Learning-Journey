# ðŸ”’ Day 24: Debugging & Security in AI Apps

## ðŸŽ¯ Learning Objectives
- Detect & mitigate **prompt injection** patterns.
- Enforce **context-only answering** for RAG.
- Add **output validation** (basic JSON mode).
- Log **decisions & refusals** for debugging.

## ðŸ§© What Youâ€™ll Build
A secure Q&A helper that:
1) Loads a small in-memory knowledge base,
2) Retrieves the top-k relevant chunks,
3) **Sanitises** user input, blocks suspicious instructions,
4) Forces **context-only** answers,
5) Optionally **validates JSON** outputs.

## ðŸ”§ Setup
```bash
pip install -r requirements.txt
# Optional envs:
# export LMSTUDIO_API_URL="http://localhost:1234/v1/completions"
# export LMSTUDIO_MODEL_ID="mistralai/Mistral-7B-Instruct-v0.1"
